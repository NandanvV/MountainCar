# ğŸš— MountainCar Reinforcement Learning

This project implements **Q-learning** and **SARSA** to solve the **MountainCar-v0** environment using **OpenAI Gym**. The agent learns to drive up a hill by optimizing its policy using reinforcement learning.

## ğŸ”¹ Features
- **Q-learning & SARSA** for reinforcement learning training
- **Hyperparameter tuning via Grid Search**
- **User-friendly menu interface (`main.py`) for easy execution**
- **Modular structure for easy extension to other RL algorithms**
- **Configurable hyperparameters via `config.py`**

---

## ğŸ›  Installation
Ensure you have **Python 3.10+** installed, then install the required dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running the Project
### **Start the Interactive Menu**
Everythingâ€”training, testing, and hyperparameter tuningâ€”can be accessed via:

```bash
python main.py
```

You'll be presented with a **menu** where you can:  
1ï¸âƒ£ **Testing before learning (Random actions)**  
2ï¸âƒ£ **Train using Q-learning**  
3ï¸âƒ£ **Train using SARSA**  
4ï¸âƒ£ **Grid Search for Q-learning**  
5ï¸âƒ£ **Grid Search for SARSA**  
6ï¸âƒ£ **Quit**  

Simply **enter the number** corresponding to your desired action.

---

## ğŸ”§ Modifying Hyperparameters
The **default hyperparameters** are stored in `config.py`. You can **manually update them** before training. The current hyperparamters are the optimal hyperparamters found with grid search.

The program will automatically use the updated hyperparameters.

---

## ğŸ“Š Running Grid Search for Best Hyperparameters
To find the optimal hyperparameters using **grid search**, run grid search from the menu.

- The best hyperparameters will be printed and stored into a json file.
- You can manually update `config.py` with these values.
- Grid search **does NOT automatically modify `config.py`**.

---
